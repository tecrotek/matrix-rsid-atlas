# Evidence Log: HCU-CTX-0001 (Axiom Translator)

> **Status:** VERIFIED
> **Task:** Implement "Scaled Dot-Product Attention" (Vaswani et al., 2017) from LaTeX.
> **Input:** $Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$

---

## 1. Deconstruction (AI Reasoning)
*Generated by HCU-CTX-0001*

1.  **Dimensionality Check:**
    *   Must handle Batch ($B$), Sequence ($L$), and Embedding ($D$) dimensions.
    *   Numerical Stability: Division by $\sqrt{d_k}$ is critical for gradient stability.
2.  **Execution Path:** Deconstruct -> Map -> Code -> Verify.

---

## 2. Execution Artifact (Result)

### Notation & Dimensionality Map
| Symbol | Variable Name | Type | Shape | Description |
| :--- | :--- | :--- | :--- | :--- |
| $Q$ | `query` | Tensor | $(B, H, L, d_k)$ | Query vectors (Batch, Heads, Seq, Dim) |
| $K$ | `key` | Tensor | $(B, H, S, d_k)$ | Key vectors (Target Sequence length $S$) |
| $V$ | `value` | Tensor | $(B, H, S, d_v)$ | Value vectors |
| $d_k$ | `d_k` | int | Scalar | Scaling factor (dimension of keys) |
| $QK^T$ | `scores` | Tensor | $(B, H, L, S)$ | Raw attention scores (Logits) |

### Production-Grade Implementation (PyTorch)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class ScaledDotProductAttention(nn.Module):
    """
    Implements Equation: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
    Physics: Relational coupling between entities weighted by compatibility.
    """
    def __init__(self, dropout_rate: float = 0.0):
        super().__init__()
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        """
        Args:
            query: (Batch, Heads, Seq_Len_Q, d_k)
            key:   (Batch, Heads, Seq_Len_K, d_k)
            value: (Batch, Heads, Seq_Len_K, d_v)
            mask:  (Batch, 1, 1, Seq_Len_K) - Optional mask (e.g. for padding)
        """
        d_k = query.size(-1)
        
        # 1. Matrix Multiplication (Interaction)
        # (B, H, L, d_k) @ (B, H, d_k, S) -> (B, H, L, S)
        scores = torch.matmul(query, key.transpose(-2, -1))
        
        # 2. Scaling (Stability Guard)
        # Prevents gradients from vanishing in Softmax for large d_k
        scores = scores / math.sqrt(d_k)
        
        # 3. Masking (Topology Constraint)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9) # -inf for softmax
        
        # 4. Probability Distribution (Normalization)
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # 5. Weighted Aggregation (Synthesis)
        # (B, H, L, S) @ (B, H, S, d_v) -> (B, H, L, d_v)
        output = torch.matmul(attn_weights, value)
        
        return output, attn_weights

# --- SANITY CHECK BLOCK ---
def run_sanity_check():
    print("/// HCU-CTX-0001 DIAGNOSTICS ///")
    B, H, L, S, D_k, D_v = 2, 4, 10, 10, 64, 64
    
    # Mock Data
    q = torch.randn(B, H, L, D_k)
    k = torch.randn(B, H, S, D_k)
    v = torch.randn(B, H, S, D_v)
    
    model = ScaledDotProductAttention()
    out, weights = model(q, k, v)
    
    # Assertions (Physics Verification)
    expected_shape = (B, H, L, D_v)
    assert out.shape == expected_shape, f"Shape Mismatch: {out.shape} != {expected_shape}"
    
    # Probability Axiom Check: Sum of weights must be 1.0
    sum_check = weights.sum(dim=-1)
    assert torch.allclose(sum_check, torch.ones_like(sum_check), atol=1e-6), "Softmax violation: Sum != 1"
    
    print(f"[PASS] Output Shape: {out.shape}")
    print(f"[PASS] Probability Axiom Verified.")
    print("Status: READY FOR DEPLOYMENT.")

if __name__ == "__main__":
    run_sanity_check()
```

### 3. Complexity Analysis
*   **Time Complexity:** $O(L \cdot S \cdot d_k)$
*   **Space Complexity:** $O(L \cdot S)$
